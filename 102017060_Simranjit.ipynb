{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 \n",
    "Balancing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(772, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0     0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1     0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2     1 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3     1 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4     2 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      1  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input features and target\n",
    "y = df.Class\n",
    "X = df.drop('Class', axis=1)\n",
    "\n",
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>427</td>\n",
       "      <td>0.823788</td>\n",
       "      <td>-0.902938</td>\n",
       "      <td>0.107280</td>\n",
       "      <td>0.384451</td>\n",
       "      <td>-0.790005</td>\n",
       "      <td>-0.431549</td>\n",
       "      <td>0.114447</td>\n",
       "      <td>-0.124467</td>\n",
       "      <td>0.706185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203972</td>\n",
       "      <td>-0.995558</td>\n",
       "      <td>-0.173509</td>\n",
       "      <td>-0.063200</td>\n",
       "      <td>0.180529</td>\n",
       "      <td>0.848548</td>\n",
       "      <td>-0.115666</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>246.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>503</td>\n",
       "      <td>0.860545</td>\n",
       "      <td>-0.747804</td>\n",
       "      <td>-0.955151</td>\n",
       "      <td>-1.124056</td>\n",
       "      <td>0.186536</td>\n",
       "      <td>-0.385942</td>\n",
       "      <td>0.703443</td>\n",
       "      <td>-0.282689</td>\n",
       "      <td>0.482600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236279</td>\n",
       "      <td>-1.185203</td>\n",
       "      <td>-0.329844</td>\n",
       "      <td>-1.027355</td>\n",
       "      <td>0.613308</td>\n",
       "      <td>-0.979857</td>\n",
       "      <td>-0.018398</td>\n",
       "      <td>0.046621</td>\n",
       "      <td>268.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>323</td>\n",
       "      <td>-0.704133</td>\n",
       "      <td>0.341397</td>\n",
       "      <td>1.740027</td>\n",
       "      <td>-1.661595</td>\n",
       "      <td>0.872313</td>\n",
       "      <td>-0.007311</td>\n",
       "      <td>0.923083</td>\n",
       "      <td>-0.575939</td>\n",
       "      <td>0.447697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206984</td>\n",
       "      <td>-0.321045</td>\n",
       "      <td>-0.334626</td>\n",
       "      <td>-0.813176</td>\n",
       "      <td>-0.265089</td>\n",
       "      <td>0.689043</td>\n",
       "      <td>-0.904113</td>\n",
       "      <td>-0.579831</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>382</td>\n",
       "      <td>-0.804275</td>\n",
       "      <td>0.379523</td>\n",
       "      <td>1.892525</td>\n",
       "      <td>0.315886</td>\n",
       "      <td>-0.194783</td>\n",
       "      <td>-0.248984</td>\n",
       "      <td>0.422565</td>\n",
       "      <td>-0.028321</td>\n",
       "      <td>-0.247661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152900</td>\n",
       "      <td>-0.213726</td>\n",
       "      <td>0.174706</td>\n",
       "      <td>0.486816</td>\n",
       "      <td>-0.365680</td>\n",
       "      <td>0.963301</td>\n",
       "      <td>-0.045545</td>\n",
       "      <td>0.110528</td>\n",
       "      <td>45.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>27</td>\n",
       "      <td>-1.452187</td>\n",
       "      <td>1.765124</td>\n",
       "      <td>0.611669</td>\n",
       "      <td>1.176825</td>\n",
       "      <td>-0.445980</td>\n",
       "      <td>0.246826</td>\n",
       "      <td>-0.257566</td>\n",
       "      <td>1.092472</td>\n",
       "      <td>-0.607524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082280</td>\n",
       "      <td>0.325782</td>\n",
       "      <td>-0.069107</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>-0.044668</td>\n",
       "      <td>-0.243441</td>\n",
       "      <td>0.149180</td>\n",
       "      <td>0.120557</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "573   427  0.823788 -0.902938  0.107280  0.384451 -0.790005 -0.431549   \n",
       "665   503  0.860545 -0.747804 -0.955151 -1.124056  0.186536 -0.385942   \n",
       "445   323 -0.704133  0.341397  1.740027 -1.661595  0.872313 -0.007311   \n",
       "516   382 -0.804275  0.379523  1.892525  0.315886 -0.194783 -0.248984   \n",
       "37     27 -1.452187  1.765124  0.611669  1.176825 -0.445980  0.246826   \n",
       "\n",
       "           V7        V8        V9  ...       V21       V22       V23  \\\n",
       "573  0.114447 -0.124467  0.706185  ... -0.203972 -0.995558 -0.173509   \n",
       "665  0.703443 -0.282689  0.482600  ... -0.236279 -1.185203 -0.329844   \n",
       "445  0.923083 -0.575939  0.447697  ... -0.206984 -0.321045 -0.334626   \n",
       "516  0.422565 -0.028321 -0.247661  ... -0.152900 -0.213726  0.174706   \n",
       "37  -0.257566  1.092472 -0.607524  ...  0.082280  0.325782 -0.069107   \n",
       "\n",
       "          V24       V25       V26       V27       V28  Amount  Class  \n",
       "573 -0.063200  0.180529  0.848548 -0.115666  0.041674  246.96      0  \n",
       "665 -1.027355  0.613308 -0.979857 -0.018398  0.046621  268.93      0  \n",
       "445 -0.813176 -0.265089  0.689043 -0.904113 -0.579831    0.77      0  \n",
       "516  0.486816 -0.365680  0.963301 -0.045545  0.110528   45.38      0  \n",
       "37   0.020962 -0.044668 -0.243441  0.149180  0.120557    1.80      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate our training data back together\n",
    "X = pd.concat([X_train, y_train], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    573\n",
       "1    573\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate minority and majority classes\n",
    "not_fraud = X[X.Class==0]\n",
    "fraud = X[X.Class==1]\n",
    "\n",
    "# upsample minority\n",
    "fraud_upsampled = resample(fraud,\n",
    "                          replace=True, # sample with replacement\n",
    "                          n_samples=len(not_fraud), # match number in majority class\n",
    "                          random_state=27) # reproducible results\n",
    "\n",
    "# combine majority and upsampled minority\n",
    "new_df = pd.concat([not_fraud, fraud_upsampled])\n",
    "\n",
    "# check new class counts\n",
    "new_df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>427</td>\n",
       "      <td>0.823788</td>\n",
       "      <td>-0.902938</td>\n",
       "      <td>0.107280</td>\n",
       "      <td>0.384451</td>\n",
       "      <td>-0.790005</td>\n",
       "      <td>-0.431549</td>\n",
       "      <td>0.114447</td>\n",
       "      <td>-0.124467</td>\n",
       "      <td>0.706185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203972</td>\n",
       "      <td>-0.995558</td>\n",
       "      <td>-0.173509</td>\n",
       "      <td>-0.063200</td>\n",
       "      <td>0.180529</td>\n",
       "      <td>0.848548</td>\n",
       "      <td>-0.115666</td>\n",
       "      <td>0.041674</td>\n",
       "      <td>246.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>503</td>\n",
       "      <td>0.860545</td>\n",
       "      <td>-0.747804</td>\n",
       "      <td>-0.955151</td>\n",
       "      <td>-1.124056</td>\n",
       "      <td>0.186536</td>\n",
       "      <td>-0.385942</td>\n",
       "      <td>0.703443</td>\n",
       "      <td>-0.282689</td>\n",
       "      <td>0.482600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236279</td>\n",
       "      <td>-1.185203</td>\n",
       "      <td>-0.329844</td>\n",
       "      <td>-1.027355</td>\n",
       "      <td>0.613308</td>\n",
       "      <td>-0.979857</td>\n",
       "      <td>-0.018398</td>\n",
       "      <td>0.046621</td>\n",
       "      <td>268.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>323</td>\n",
       "      <td>-0.704133</td>\n",
       "      <td>0.341397</td>\n",
       "      <td>1.740027</td>\n",
       "      <td>-1.661595</td>\n",
       "      <td>0.872313</td>\n",
       "      <td>-0.007311</td>\n",
       "      <td>0.923083</td>\n",
       "      <td>-0.575939</td>\n",
       "      <td>0.447697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206984</td>\n",
       "      <td>-0.321045</td>\n",
       "      <td>-0.334626</td>\n",
       "      <td>-0.813176</td>\n",
       "      <td>-0.265089</td>\n",
       "      <td>0.689043</td>\n",
       "      <td>-0.904113</td>\n",
       "      <td>-0.579831</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>382</td>\n",
       "      <td>-0.804275</td>\n",
       "      <td>0.379523</td>\n",
       "      <td>1.892525</td>\n",
       "      <td>0.315886</td>\n",
       "      <td>-0.194783</td>\n",
       "      <td>-0.248984</td>\n",
       "      <td>0.422565</td>\n",
       "      <td>-0.028321</td>\n",
       "      <td>-0.247661</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152900</td>\n",
       "      <td>-0.213726</td>\n",
       "      <td>0.174706</td>\n",
       "      <td>0.486816</td>\n",
       "      <td>-0.365680</td>\n",
       "      <td>0.963301</td>\n",
       "      <td>-0.045545</td>\n",
       "      <td>0.110528</td>\n",
       "      <td>45.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>27</td>\n",
       "      <td>-1.452187</td>\n",
       "      <td>1.765124</td>\n",
       "      <td>0.611669</td>\n",
       "      <td>1.176825</td>\n",
       "      <td>-0.445980</td>\n",
       "      <td>0.246826</td>\n",
       "      <td>-0.257566</td>\n",
       "      <td>1.092472</td>\n",
       "      <td>-0.607524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082280</td>\n",
       "      <td>0.325782</td>\n",
       "      <td>-0.069107</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>-0.044668</td>\n",
       "      <td>-0.243441</td>\n",
       "      <td>0.149180</td>\n",
       "      <td>0.120557</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "573   427  0.823788 -0.902938  0.107280  0.384451 -0.790005 -0.431549   \n",
       "665   503  0.860545 -0.747804 -0.955151 -1.124056  0.186536 -0.385942   \n",
       "445   323 -0.704133  0.341397  1.740027 -1.661595  0.872313 -0.007311   \n",
       "516   382 -0.804275  0.379523  1.892525  0.315886 -0.194783 -0.248984   \n",
       "37     27 -1.452187  1.765124  0.611669  1.176825 -0.445980  0.246826   \n",
       "\n",
       "           V7        V8        V9  ...       V21       V22       V23  \\\n",
       "573  0.114447 -0.124467  0.706185  ... -0.203972 -0.995558 -0.173509   \n",
       "665  0.703443 -0.282689  0.482600  ... -0.236279 -1.185203 -0.329844   \n",
       "445  0.923083 -0.575939  0.447697  ... -0.206984 -0.321045 -0.334626   \n",
       "516  0.422565 -0.028321 -0.247661  ... -0.152900 -0.213726  0.174706   \n",
       "37  -0.257566  1.092472 -0.607524  ...  0.082280  0.325782 -0.069107   \n",
       "\n",
       "          V24       V25       V26       V27       V28  Amount  Class  \n",
       "573 -0.063200  0.180529  0.848548 -0.115666  0.041674  246.96      0  \n",
       "665 -1.027355  0.613308 -0.979857 -0.018398  0.046621  268.93      0  \n",
       "445 -0.813176 -0.265089  0.689043 -0.904113 -0.579831    0.77      0  \n",
       "516  0.486816 -0.365680  0.963301 -0.045545  0.110528   45.38      0  \n",
       "37   0.020962 -0.044668 -0.243441  0.149180  0.120557    1.80      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the Classes in the subsample dataset\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: Class, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(new_df['Class'].value_counts()/len(new_df))\n",
    "\n",
    "sns.countplot(x='Class', data=new_df)\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - 2 \n",
    "Creating Samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Random Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caluculating the Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384.1599999999999"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "z=1.96\n",
    "p=0.5\n",
    "e=0.05\n",
    "n = ((z**2)*p*(1-p))/e**2\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def z_calc(p1, p2, n1, n2):\n",
    "    p_star = (p1*n1 + p2*n2) / (n1 + n2)\n",
    "    return (p2 - p1) / math.sqrt(p_star*(1 - p_star)*((1.0 / n1) + (1.0 / n2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample size needed for each group is 393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:6832: RuntimeWarning: invalid value encountered in _nct_sf\n",
      "  return np.clip(_boost._nct_sf(x, df, nc), 0, 1)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py:6826: RuntimeWarning: invalid value encountered in _nct_cdf\n",
      "  return np.clip(_boost._nct_cdf(x, df, nc), 0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Initiate the power analysis\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "power_analysis = TTestIndPower()\n",
    "# Calculate sample size\n",
    "sample_size = power_analysis.solve_power(effect_size = 0.2, alpha = 0.05, power = 0.8, alternative = 'two-sided')\n",
    "# Print results\n",
    "print('The sample size needed for each group is', round(sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time        V1        V2        V3        V4        V5        V6  \\\n",
      "172   110 -0.591933  0.111273  0.699126 -1.536074  1.193208  0.648896   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "639   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
      "717   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n",
      "244   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
      "..    ...       ...       ...       ...       ...       ...       ...   \n",
      "203   135  1.207207  0.241318  0.258745  0.653335 -0.256426 -0.620309   \n",
      "121    77 -0.427191  0.745708  1.761811 -0.165130  0.058298 -0.213413   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "469   346  1.507578 -1.092820  0.360102 -1.770940 -0.913849  0.678723   \n",
      "658   497 -0.417836  0.981103  1.135820 -0.172593  0.396675 -0.002842   \n",
      "\n",
      "           V7        V8        V9  ...       V21       V22       V23  \\\n",
      "172  0.796706  0.016904  0.789664  ...  0.205964  1.216195  0.093396   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "639  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n",
      "717  0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n",
      "244  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "203 -0.039653 -0.022139  0.145191  ... -0.267165 -0.780158  0.165657   \n",
      "121  0.647323  0.073464 -0.291864  ... -0.201681 -0.432070  0.013164   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "469 -1.380228  0.147690 -1.970261  ... -0.047078  0.360796 -0.137631   \n",
      "658  0.519780  0.248763 -0.593497  ... -0.205580 -0.547474 -0.020486   \n",
      "\n",
      "          V24       V25       V26       V27       V28  Amount  Class  \n",
      "172 -0.900310 -0.423966 -0.607857  0.017705 -0.076746   13.95      0  \n",
      "623 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
      "639  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n",
      "717 -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n",
      "244 -1.363967 -1.389079  0.075412  0.231750  0.230171    0.99      1  \n",
      "..        ...       ...       ...       ...       ...     ...    ...  \n",
      "203  0.012956  0.116176  0.126406 -0.012156  0.026239    4.57      0  \n",
      "121  0.161606 -0.401310  0.047423  0.102549 -0.116571    9.12      0  \n",
      "623 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
      "469 -1.122472  0.438241 -0.022090  0.072569  0.003450    6.00      0  \n",
      "658 -0.368187 -0.227053  0.105281  0.253800  0.081276    8.83      0  \n",
      "\n",
      "[393 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sample_size = 393\n",
    "\n",
    "sample1 = new_df.sample(n=sample_size, random_state=0)\n",
    "print(sample1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Systematic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time        V1        V2        V3        V4        V5        V6  \\\n",
      "573   427  0.823788 -0.902938  0.107280  0.384451 -0.790005 -0.431549   \n",
      "516   382 -0.804275  0.379523  1.892525  0.315886 -0.194783 -0.248984   \n",
      "413   298  1.218417  0.077792  0.533944  0.843849 -0.536022 -0.702281   \n",
      "315   227  1.120599 -0.313308  0.395309  0.596756 -0.420412  0.325862   \n",
      "415   300  0.007043  1.239504  0.417050  0.530004  0.942324 -0.559968   \n",
      "..    ...       ...       ...       ...       ...       ...       ...   \n",
      "639   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "699   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "\n",
      "           V7        V8        V9  ...       V21       V22       V23  \\\n",
      "573  0.114447 -0.124467  0.706185  ... -0.203972 -0.995558 -0.173509   \n",
      "516  0.422565 -0.028321 -0.247661  ... -0.152900 -0.213726  0.174706   \n",
      "413  0.006656 -0.153411  0.421584  ... -0.224558 -0.466722  0.003663   \n",
      "315 -0.404105  0.283071  0.630035  ... -0.039016  0.005732 -0.108844   \n",
      "415  1.502981 -0.751563  0.012713  ... -0.046722  0.720870 -0.141873   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "639  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "699 -2.110183  0.788347  0.958809  ...  0.422452  1.195394  0.297836   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "\n",
      "          V24       V25       V26       V27       V28  Amount  Class  \n",
      "573 -0.063200  0.180529  0.848548 -0.115666  0.041674  246.96      0  \n",
      "516  0.486816 -0.365680  0.963301 -0.045545  0.110528   45.38      0  \n",
      "413  0.438026  0.460760  0.267056 -0.021916  0.014812   13.99      0  \n",
      "315 -0.283233  0.441306  0.579529 -0.026439 -0.007082   29.95      0  \n",
      "415  0.019747 -0.665861 -0.489940  0.157919 -0.332123    1.91      0  \n",
      "..        ...       ...       ...       ...       ...     ...    ...  \n",
      "639  0.385384  0.391117 -0.453853 -0.104448 -0.125765    1.00      1  \n",
      "623 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
      "623 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
      "699 -0.857105 -0.219322  0.861019 -0.124622 -0.171060    1.50      1  \n",
      "623 -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
      "\n",
      "[382 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "# Calculate the number of rows in the dataset\n",
    "n = len(new_df)\n",
    "\n",
    "# Set the sampling interval \"k\" as the square root of the number of rows in the dataset\n",
    "k = 3\n",
    "\n",
    "# Select every \"k\" row starting from a random index in the dataset\n",
    "sample2 = new_df.iloc[::k]\n",
    "\n",
    "# Print the first few rows of the sample\n",
    "print(sample2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time        V1        V2        V3        V4        V5        V6  \\\n",
      "172   110 -0.591933  0.111273  0.699126 -1.536074  1.193208  0.648896   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "639   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
      "717   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n",
      "244   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
      "..    ...       ...       ...       ...       ...       ...       ...   \n",
      "639   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
      "367   268  1.146065  0.285853  0.562439  1.459336 -0.225891 -0.346303   \n",
      "699   529 -2.000567 -2.495484  2.467149  1.140053  2.462010  0.594262   \n",
      "497   366  1.020399 -0.639479  0.941568  0.121307 -0.996320  0.177803   \n",
      "74     48 -1.793406  1.854604  0.979514  1.112262 -0.206403 -0.199546   \n",
      "\n",
      "           V7        V8        V9  ...       V22       V23       V24  \\\n",
      "172  0.796706  0.016904  0.789664  ...  1.216195  0.093396 -0.900310   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.435477  1.375966 -0.293803   \n",
      "639  0.879016 -0.156590 -0.142117  ...  0.281378 -0.257966  0.385384   \n",
      "717  0.269083  0.140631  0.023464  ... -0.192036 -0.261879 -0.237477   \n",
      "244  0.296319  0.139497 -0.123050  ... -0.381932  0.151012 -1.363967   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "639  0.879016 -0.156590 -0.142117  ...  0.281378 -0.257966  0.385384   \n",
      "367  0.131988 -0.085179  0.136365  ...  0.076177 -0.030567  0.444843   \n",
      "699 -2.110183  0.788347  0.958809  ...  1.195394  0.297836 -0.857105   \n",
      "497 -0.589656  0.153583  0.835979  ... -0.341663 -0.060548  0.116672   \n",
      "74   0.616642  0.064180  0.690272  ...  0.235138 -0.091442  0.417654   \n",
      "\n",
      "          V25       V26       V27       V28  Amount  Class  cluster  \n",
      "172 -0.423966 -0.607857  0.017705 -0.076746   13.95      0        0  \n",
      "623  0.279798 -0.145362 -0.252773  0.035764  529.00      1        0  \n",
      "639  0.391117 -0.453853 -0.104448 -0.125765    1.00      1        0  \n",
      "717 -0.335040  0.240323 -0.345129 -0.383563    1.00      1        0  \n",
      "244 -1.389079  0.075412  0.231750  0.230171    0.99      1        0  \n",
      "..        ...       ...       ...       ...     ...    ...      ...  \n",
      "639  0.391117 -0.453853 -0.104448 -0.125765    1.00      1        0  \n",
      "367  0.650037 -0.335736  0.046372  0.020899    6.54      0        0  \n",
      "699 -0.219322  0.861019 -0.124622 -0.171060    1.50      1        0  \n",
      "497  0.144624  0.932850 -0.049522  0.018131   97.47      0        0  \n",
      "74   0.122734 -0.232985  0.811880  0.618568   26.72      0        0  \n",
      "\n",
      "[600 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_clustered_Sample(df, n_per_cluster, num_select_clusters):\n",
    "    N = len(df)\n",
    "    K = int(N/n_per_cluster)\n",
    "    data = None\n",
    "    for k in range(K):\n",
    "        sample_k = df.sample(n_per_cluster)\n",
    "        sample_k[\"cluster\"] = np.repeat(k,len(sample_k))\n",
    "        df = df.drop(index = sample_k.index)\n",
    "        data = pd.concat([data,sample_k],axis = 0)\n",
    "\n",
    "    random_chosen_clusters = np.random.randint(0,K,size = num_select_clusters)\n",
    "    samples = data[data.cluster.isin(random_chosen_clusters)]\n",
    "    return(samples)\n",
    "\n",
    "sample3 = get_clustered_Sample(df = new_df, n_per_cluster = 600, num_select_clusters = 2)\n",
    "print(sample3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Time        V1        V2        V3        V4        V5        V6  \\\n",
      "51     36 -1.004929 -0.985978 -0.038039  3.710061 -6.631951  5.122103   \n",
      "713   537 -1.097278  1.094348  2.074334 -0.085776 -0.200635 -0.424142   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "623   472 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
      "..    ...       ...       ...       ...       ...       ...       ...   \n",
      "123    78 -0.291241  1.515688  1.514355  2.518365  0.583648  0.425561   \n",
      "6       4  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
      "244   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
      "22     18  1.166616  0.502120 -0.067300  2.261569  0.428804  0.089474   \n",
      "639   484 -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004   \n",
      "\n",
      "           V7        V8        V9  ...       V21       V22       V23  \\\n",
      "51   4.371691 -2.006868 -0.278736  ...  1.393406 -0.381671  0.969719   \n",
      "713  0.575308  0.106945 -0.278105  ... -0.080692 -0.055630 -0.132471   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "623  0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "123  0.598690  0.056754 -1.475621  ... -0.252659 -0.613055 -0.126407   \n",
      "6   -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104   \n",
      "244  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n",
      "22   0.241147  0.138082 -0.989162  ...  0.018702 -0.061972 -0.103855   \n",
      "639  0.879016 -0.156590 -0.142117  ...  0.066353  0.281378 -0.257966   \n",
      "\n",
      "          V24       V25       V26       V27       V28   Amount  Class  \n",
      "51   0.019445  0.570923  0.333278  0.857373 -0.075538  1402.95      0  \n",
      "713  0.410348  0.181198  0.295576  0.395454  0.197642    27.34      0  \n",
      "623 -0.293803  0.279798 -0.145362 -0.252773  0.035764   529.00      1  \n",
      "623 -0.293803  0.279798 -0.145362 -0.252773  0.035764   529.00      1  \n",
      "623 -0.293803  0.279798 -0.145362 -0.252773  0.035764   529.00      1  \n",
      "..        ...       ...       ...       ...       ...      ...    ...  \n",
      "123 -0.636697 -0.255688  0.019571  0.382581  0.194255     4.68      0  \n",
      "6   -0.780055  0.750137 -0.257237  0.034507  0.005168     4.99      0  \n",
      "244 -1.363967 -1.389079  0.075412  0.231750  0.230171     0.99      1  \n",
      "22  -0.370415  0.603200  0.108556 -0.040521 -0.011418     2.28      0  \n",
      "639  0.385384  0.391117 -0.453853 -0.104448 -0.125765     1.00      1  \n",
      "\n",
      "[670 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample4 = new_df.sample(n=670, weights=\"Amount\")\n",
    "print(sample4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = ss(test_size=0.1)\n",
    "def sample_stratified(dataframe, size, variable):\n",
    "    sp = ss(test_size=size)\n",
    "    for _, y in sp.split(dataframe, dataframe[variable]):\n",
    "        dataframey = dataframe.iloc[y]\n",
    "    return dataframey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>218</td>\n",
       "      <td>1.201989</td>\n",
       "      <td>-0.034452</td>\n",
       "      <td>0.696628</td>\n",
       "      <td>0.956746</td>\n",
       "      <td>-0.005827</td>\n",
       "      <td>1.095223</td>\n",
       "      <td>-0.574524</td>\n",
       "      <td>0.243435</td>\n",
       "      <td>0.788419</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109658</td>\n",
       "      <td>-0.011904</td>\n",
       "      <td>-0.166958</td>\n",
       "      <td>-1.295060</td>\n",
       "      <td>0.570793</td>\n",
       "      <td>-0.296854</td>\n",
       "      <td>0.094955</td>\n",
       "      <td>0.024008</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>204</td>\n",
       "      <td>-0.188424</td>\n",
       "      <td>0.877602</td>\n",
       "      <td>-0.734686</td>\n",
       "      <td>-0.913404</td>\n",
       "      <td>1.941770</td>\n",
       "      <td>4.037423</td>\n",
       "      <td>-1.707118</td>\n",
       "      <td>-2.537641</td>\n",
       "      <td>-0.590338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.824079</td>\n",
       "      <td>-1.042331</td>\n",
       "      <td>0.168265</td>\n",
       "      <td>0.963855</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.171167</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.164538</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>322</td>\n",
       "      <td>1.202007</td>\n",
       "      <td>0.108213</td>\n",
       "      <td>0.591724</td>\n",
       "      <td>0.566079</td>\n",
       "      <td>-0.641573</td>\n",
       "      <td>-0.816974</td>\n",
       "      <td>-0.127403</td>\n",
       "      <td>0.004375</td>\n",
       "      <td>-0.026148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.207683</td>\n",
       "      <td>-0.703844</td>\n",
       "      <td>0.165350</td>\n",
       "      <td>0.493736</td>\n",
       "      <td>0.114822</td>\n",
       "      <td>0.068947</td>\n",
       "      <td>-0.041350</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>164</td>\n",
       "      <td>0.073497</td>\n",
       "      <td>0.551033</td>\n",
       "      <td>0.451890</td>\n",
       "      <td>0.114964</td>\n",
       "      <td>0.822947</td>\n",
       "      <td>0.251480</td>\n",
       "      <td>0.296319</td>\n",
       "      <td>0.139497</td>\n",
       "      <td>-0.123050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128758</td>\n",
       "      <td>-0.381932</td>\n",
       "      <td>0.151012</td>\n",
       "      <td>-1.363967</td>\n",
       "      <td>-1.389079</td>\n",
       "      <td>0.075412</td>\n",
       "      <td>0.231750</td>\n",
       "      <td>0.230171</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>300</td>\n",
       "      <td>0.007043</td>\n",
       "      <td>1.239504</td>\n",
       "      <td>0.417050</td>\n",
       "      <td>0.530004</td>\n",
       "      <td>0.942324</td>\n",
       "      <td>-0.559968</td>\n",
       "      <td>1.502981</td>\n",
       "      <td>-0.751563</td>\n",
       "      <td>0.012713</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046722</td>\n",
       "      <td>0.720870</td>\n",
       "      <td>-0.141873</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>-0.665861</td>\n",
       "      <td>-0.489940</td>\n",
       "      <td>0.157919</td>\n",
       "      <td>-0.332123</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>539</td>\n",
       "      <td>-1.738582</td>\n",
       "      <td>0.052740</td>\n",
       "      <td>1.187057</td>\n",
       "      <td>-0.656652</td>\n",
       "      <td>0.920623</td>\n",
       "      <td>-0.291788</td>\n",
       "      <td>0.269083</td>\n",
       "      <td>0.140631</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179545</td>\n",
       "      <td>-0.192036</td>\n",
       "      <td>-0.261879</td>\n",
       "      <td>-0.237477</td>\n",
       "      <td>-0.335040</td>\n",
       "      <td>0.240323</td>\n",
       "      <td>-0.345129</td>\n",
       "      <td>-0.383563</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>539</td>\n",
       "      <td>-1.738582</td>\n",
       "      <td>0.052740</td>\n",
       "      <td>1.187057</td>\n",
       "      <td>-0.656652</td>\n",
       "      <td>0.920623</td>\n",
       "      <td>-0.291788</td>\n",
       "      <td>0.269083</td>\n",
       "      <td>0.140631</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179545</td>\n",
       "      <td>-0.192036</td>\n",
       "      <td>-0.261879</td>\n",
       "      <td>-0.237477</td>\n",
       "      <td>-0.335040</td>\n",
       "      <td>0.240323</td>\n",
       "      <td>-0.345129</td>\n",
       "      <td>-0.383563</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>241</td>\n",
       "      <td>-1.142321</td>\n",
       "      <td>0.626405</td>\n",
       "      <td>2.526917</td>\n",
       "      <td>2.827973</td>\n",
       "      <td>0.619263</td>\n",
       "      <td>0.897473</td>\n",
       "      <td>0.536278</td>\n",
       "      <td>-0.060163</td>\n",
       "      <td>-0.813749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309746</td>\n",
       "      <td>-0.269173</td>\n",
       "      <td>0.177396</td>\n",
       "      <td>-0.019578</td>\n",
       "      <td>0.048651</td>\n",
       "      <td>0.068831</td>\n",
       "      <td>-0.246503</td>\n",
       "      <td>-0.230837</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>493</td>\n",
       "      <td>1.219725</td>\n",
       "      <td>-0.481149</td>\n",
       "      <td>-0.324351</td>\n",
       "      <td>-1.552562</td>\n",
       "      <td>-0.258357</td>\n",
       "      <td>-0.519342</td>\n",
       "      <td>-0.035458</td>\n",
       "      <td>-0.117824</td>\n",
       "      <td>1.267125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040428</td>\n",
       "      <td>-0.106938</td>\n",
       "      <td>-0.171758</td>\n",
       "      <td>-0.721597</td>\n",
       "      <td>0.525568</td>\n",
       "      <td>0.084212</td>\n",
       "      <td>-0.001621</td>\n",
       "      <td>0.011866</td>\n",
       "      <td>67.94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time        V1        V2        V3        V4        V5        V6  \\\n",
       "303   218  1.201989 -0.034452  0.696628  0.956746 -0.005827  1.095223   \n",
       "285   204 -0.188424  0.877602 -0.734686 -0.913404  1.941770  4.037423   \n",
       "442   322  1.202007  0.108213  0.591724  0.566079 -0.641573 -0.816974   \n",
       "1       0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "244   164  0.073497  0.551033  0.451890  0.114964  0.822947  0.251480   \n",
       "..    ...       ...       ...       ...       ...       ...       ...   \n",
       "415   300  0.007043  1.239504  0.417050  0.530004  0.942324 -0.559968   \n",
       "717   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n",
       "717   539 -1.738582  0.052740  1.187057 -0.656652  0.920623 -0.291788   \n",
       "328   241 -1.142321  0.626405  2.526917  2.827973  0.619263  0.897473   \n",
       "652   493  1.219725 -0.481149 -0.324351 -1.552562 -0.258357 -0.519342   \n",
       "\n",
       "           V7        V8        V9  ...       V21       V22       V23  \\\n",
       "303 -0.574524  0.243435  0.788419  ... -0.109658 -0.011904 -0.166958   \n",
       "285 -1.707118 -2.537641 -0.590338  ... -0.824079 -1.042331  0.168265   \n",
       "442 -0.127403  0.004375 -0.026148  ... -0.207683 -0.703844  0.165350   \n",
       "1   -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "244  0.296319  0.139497 -0.123050  ... -0.128758 -0.381932  0.151012   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "415  1.502981 -0.751563  0.012713  ... -0.046722  0.720870 -0.141873   \n",
       "717  0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n",
       "717  0.269083  0.140631  0.023464  ... -0.179545 -0.192036 -0.261879   \n",
       "328  0.536278 -0.060163 -0.813749  ... -0.309746 -0.269173  0.177396   \n",
       "652 -0.035458 -0.117824  1.267125  ... -0.040428 -0.106938 -0.171758   \n",
       "\n",
       "          V24       V25       V26       V27       V28  Amount  Class  \n",
       "303 -1.295060  0.570793 -0.296854  0.094955  0.024008   12.99      0  \n",
       "285  0.963855  0.887850  0.171167  0.012284  0.164538    1.98      0  \n",
       "442  0.493736  0.114822  0.068947 -0.041350  0.008041    1.79      0  \n",
       "1   -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      1  \n",
       "244 -1.363967 -1.389079  0.075412  0.231750  0.230171    0.99      1  \n",
       "..        ...       ...       ...       ...       ...     ...    ...  \n",
       "415  0.019747 -0.665861 -0.489940  0.157919 -0.332123    1.91      0  \n",
       "717 -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n",
       "717 -0.237477 -0.335040  0.240323 -0.345129 -0.383563    1.00      1  \n",
       "328 -0.019578  0.048651  0.068831 -0.246503 -0.230837   10.62      0  \n",
       "652 -0.721597  0.525568  0.084212 -0.001621  0.011866   67.94      0  \n",
       "\n",
       "[573 rows x 31 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample5 = sample_stratified(new_df, 0.5, 'Class')\n",
    "sample5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - 3\n",
    "Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample1.drop('Class', axis=1)\n",
    "y = sample1['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers:  LogisticRegression Has a training score of 92.0 % accuracy score\n",
      "Classifiers:  KNeighborsClassifier Has a training score of 94.0 % accuracy score\n",
      "Classifiers:  SVC Has a training score of 76.0 % accuracy score\n",
      "Classifiers:  DecisionTreeClassifier Has a training score of 97.0 % accuracy score\n",
      "Classifiers:  GaussianNB Has a training score of 88.0 % accuracy score\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "acc=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc.append(round(training_score.mean(), 2) * 100)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92.0, 94.0, 76.0, 97.0, 88.0]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample2.drop('Class', axis=1)\n",
    "y = sample2['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers:  LogisticRegression Has a training score of 95.0 % accuracy score\n",
      "Classifiers:  KNeighborsClassifier Has a training score of 94.0 % accuracy score\n",
      "Classifiers:  SVC Has a training score of 76.0 % accuracy score\n",
      "Classifiers:  DecisionTreeClassifier Has a training score of 98.0 % accuracy score\n",
      "Classifiers:  GaussianNB Has a training score of 83.0 % accuracy score\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "acc1=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc1.append(round(training_score.mean(), 2) * 100)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample3.drop('Class', axis=1)\n",
    "y = sample3['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers:  LogisticRegression Has a training score of 91.0 % accuracy score\n",
      "Classifiers:  KNeighborsClassifier Has a training score of 97.0 % accuracy score\n",
      "Classifiers:  SVC Has a training score of 78.0 % accuracy score\n",
      "Classifiers:  DecisionTreeClassifier Has a training score of 98.0 % accuracy score\n",
      "Classifiers:  GaussianNB Has a training score of 88.0 % accuracy score\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "acc2=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc2.append(round(training_score.mean(), 2) * 100)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample4.drop('Class', axis=1)\n",
    "y = sample4['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers:  LogisticRegression Has a training score of 90.0 % accuracy score\n",
      "Classifiers:  KNeighborsClassifier Has a training score of 97.0 % accuracy score\n",
      "Classifiers:  SVC Has a training score of 82.0 % accuracy score\n",
      "Classifiers:  DecisionTreeClassifier Has a training score of 99.0 % accuracy score\n",
      "Classifiers:  GaussianNB Has a training score of 82.0 % accuracy score\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "acc3=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc3.append(round(training_score.mean(), 2) * 100)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample5.drop('Class', axis=1)\n",
    "y = sample5['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers:  LogisticRegression Has a training score of 93.0 % accuracy score\n",
      "Classifiers:  KNeighborsClassifier Has a training score of 96.0 % accuracy score\n",
      "Classifiers:  SVC Has a training score of 77.0 % accuracy score\n",
      "Classifiers:  DecisionTreeClassifier Has a training score of 98.0 % accuracy score\n",
      "Classifiers:  GaussianNB Has a training score of 84.0 % accuracy score\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "acc5=[]\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    acc5.append(round(training_score.mean(), 2) * 100)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[93.0, 96.0, 77.0, 98.0, 84.0]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model      Sampling1    Sampling2    Sampling3    Sampling4    Sampling5\n",
      "-------  -----------  -----------  -----------  -----------  -----------\n",
      "M1                92           95           91           90           93\n",
      "M2                94           94           97           97           96\n",
      "M3                76           76           78           82           77\n",
      "M4                97           98           98           99           98\n",
      "M5                88           83           88           82           84\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "data = [[\"M1\",acc[0],acc1[0],acc2[0],acc3[0],acc5[0]], [\"M2\",acc[1],acc1[1],acc2[1],acc3[1],acc5[1]], [\"M3\",acc[2],acc1[2],acc2[2],acc3[2],acc5[2]], [\"M4\",acc[3],acc1[3],acc2[3],acc3[3],acc5[3]],[\"M5\",acc[4],acc1[4],acc2[4],acc3[4],acc5[4]]]\n",
    "  \n",
    "#define header names\n",
    "col_names = [\"Model\",\"Sampling1\",\"Sampling2\",\"Sampling3\",\"Sampling4\",\"Sampling5\"]\n",
    "  \n",
    "#display table\n",
    "print(tabulate(data, headers=col_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
